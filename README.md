# Generative-Models

## ğŸ“š ç›®å½•

- [æ·±åº¦å­¦ä¹ åŸºç¡€çŸ¥è¯†](#æ·±åº¦å­¦ä¹ å¿…â€œæ‡‚â€çŸ¥è¯†)
- [æ·±åº¦å­¦ä¹ ä¸‰ç§æ¶æ„](#æ·±åº¦å­¦ä¹ ä¸‰ç§æ¶æ„)
  - [CNN](#CNN)  
  - [RNN](#RNN)
  - [Transformer](#Transformer)
- [ç”Ÿæˆæ¨¡å‹æŠ€æœ¯è·¯çº¿](#ç”Ÿæˆæ¨¡å‹æŠ€æœ¯è·¯çº¿)
  - [VAE](#VAE)
  - [GAN](#GAN)
  - [Difussion](#Difussion)
  - [Autoregressive](#Autoregressive)
- [SoraæŠ€æœ¯åŸç†](#SoraæŠ€æœ¯åŸç†)


### 1.1 Sigmoid æ¿€æ´»å‡½æ•°å®ç°
![image](https://github.com/user-attachments/assets/02cf363d-cd7a-4203-8dbb-3833bcc96ff8)


```python 
import math

def sigmoid(z: float) -> float:
    result= 1 / (1 + math.exp(-z))
    return round(result, 4)

if __name__ == "__main__":
    z = float(input())
    print(f"{sigmoid(z):.4f}")
```

### 1.2 Softmax æ¿€æ´»å‡½æ•°å®ç°
![image](https://github.com/user-attachments/assets/e8799da9-e3db-4f16-a563-ae2a86a4b3c8)

```python 
import numpy as np
import math

def softmax(scores: list[float]) -> list[float]:
    exp_scores = [math.exp(score) for score in scores]
    sum_exp_scores = sum(exp_scores)

    probabilities = [round(score / sum_exp_scores, 4) for score in exp_scores]
    
    return probabilities

if __name__ == "__main__":
    scores = np.array(eval(input()))
    print(softmax(scores))
```

### 1.3 ï¼ˆå…·æœ‰åå‘ä¼ æ’­ï¼‰å•ç¥ç»å…ƒ
ä½¿ç”¨åŸºç¡€Pythonè¯­æ³•,æ²¡æœ‰ä¾èµ–NumPyç­‰åº“ï¼Œé€šè¿‡å¾ªç¯é€ä¸ªå¤„ç†æ ·æœ¬

```python 
import math
import numpy as np
def single_neuron_model(features, labels, weights, bias):
    
    probabilities = []

    for cur_feature in features:

        z = sum(weight * feature for weight, feature in zip(weights, cur_feature)) + bias
        prob = 1 / (1 + math.exp(-z))
        probabilities.append(round(prob, 4))

    mse = round(sum((prob - label)**2 for prob, label in zip(probabilities, labels)) / len(labels), 4)
    
    return probabilities, mse

def single_neuron_model(features, labels, weights, bias):
    
    features = np.array(features)
    labels = np.array(labels)
    weights = np.array(weights)

    z = np.dot(features, weights) + bias
    prob = 1 / (1 + np.exp(-z))
    probabilities = np.round(prob, 4).tolist()
    mse = np.round(np.mean((prob - labels)**2), 4)


    
    return probabilities, mse


if __name__ == "__main__":
    features = np.array(eval(input()))
    labels = np.array(eval(input()))
    weights = np.array(eval(input()))
    bias = float(input())
    print(single_neuron_model(features, labels, weights, bias))
```

å¦‚æœæ¶‰åŠåå‘ä¼ æ’­ï¼š
![image](https://github.com/user-attachments/assets/b5d437c7-ea03-46b6-bf9d-4fb56d6e63db)

```python

import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs):
    features = np.array(features)
    labels = np.array(labels)
    weight = np.array(initial_weights)
    bias = initial_bias
    mse_value = []
    for _ in range(epochs):
        z = np.dot(features, weight) + bias
        prob = sigmoid(z)

        mse = np.mean((prob - labels)**2)
        mse_value.append(round(mse, 4))

        weight_gradient = (2 / len(labels)) * np.dot(features.T, (prob - labels) * (prob * (1 - prob))) 
        bias_gradient = (2 / len(labels)) * np.sum((prob - labels)*(prob * (1 - prob)))

        weight -= learning_rate * weight_gradient
        bias -= learning_rate * bias_gradient
        updated_weight = np.round(weight, 4)
        updated_bias = round(bias, 4)

    return updated_weight.tolist(), updated_bias, mse_value

if __name__ == "__main__":
    features = np.array(eval(input()))
    labels = np.array(eval(input()))
    initial_weights = np.array(eval(input()))
    initial_bias = float(input())
    learning_rate = float(input())
    epochs = int(input())
    print(train_neuron(features, labels, initial_weights, initial_bias, learning_rate, epochs))


```

### 1.4 Log Softmaxå‡½æ•°çš„å®ç°

```python
import numpy as np


def log_softmax(scores: list) -> np.ndarray:
    scores = scores - np.max(scores)
    prob = scores - np.log(np.sum(np.exp(scores)))

    return prob

if __name__ == "__main__":
    scores = eval(input())
    print(log_softmax(scores))
```

### 1.5 ç†µã€KLæ•£åº¦ã€äº¤å‰ç†µ
![image](https://github.com/user-attachments/assets/959bf521-4046-49c5-a024-99795f02a0b4)
![image](https://github.com/user-attachments/assets/50982527-7e33-4b32-ac8f-b1af5feee25e)
![image](https://github.com/user-attachments/assets/407066ad-2417-4b42-9bd2-fab20df13679)

![image](https://github.com/user-attachments/assets/47c379dc-f7bf-44de-9fb2-7c222a95cded)

```python
import numpy as np

def kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q):
    kl_1 = np.log(sigma_q / sigma_p)
    kl_2 = (sigma_p**2 + (mu_p - mu_q)**2)/ (2*sigma_q**2)

    kl_div = kl_1 + kl_2 - 0.5
    return kl_div


if __name__ == "__main__":
    mu_p, sigma_p, mu_q, sigma_q = map(float, input().split())
    print(kl_divergence_normal(mu_p, sigma_p, mu_q, sigma_q))
```

### 1.6 ä¼˜åŒ–ç®—æ³•

æ¢¯åº¦ä¸‹é™æ˜¯ä¸€ç§ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºé€šè¿‡è¿­ä»£æ›´æ–°å‚æ•°æ¥æœ€å°åŒ–ç›®æ ‡å‡½æ•°ï¼ˆå¦‚æŸå¤±å‡½æ•°ï¼‰ï¼Œè§£å†³æ¨¡å‹è®­ç»ƒä¸­çš„å‚æ•°ä¼˜åŒ–é—®é¢˜ã€‚åŸºæœ¬æ–¹æ³•æ˜¯æ ¹æ®ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦æ–¹å‘è°ƒæ•´å‚æ•°ï¼Œå¸¸è§å˜ç§åŒ…æ‹¬ï¼šæ ‡å‡†æ¢¯åº¦ä¸‹é™ï¼ˆé€æ­¥æ²¿è´Ÿæ¢¯åº¦æ–¹å‘æ›´æ–°ï¼‰ã€éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰ï¼ˆæ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªæ ·æœ¬è®¡ç®—æ¢¯åº¦ï¼Œæå‡æ•ˆç‡ï¼‰ã€åŠ¨é‡æ³•ï¼ˆå¼•å…¥å†å²æ¢¯åº¦ç´¯ç§¯ï¼Œå‡å°‘éœ‡è¡ï¼‰ã€NAGï¼ˆNesterovåŠ é€Ÿæ¢¯åº¦ï¼‰ï¼ˆåœ¨é¢„ä¼°ä½ç½®è®¡ç®—æ¢¯åº¦ï¼Œæå‰æ„ŸçŸ¥å˜åŒ–ï¼‰ã€è‡ªé€‚åº”æ–¹æ³•ï¼ˆå¦‚ AdaGradã€RMSPropã€Adamï¼Œé€šè¿‡åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡åŠ é€Ÿæ”¶æ•›ï¼‰ã€‚è¿™äº›æ–¹æ³•æ—¨åœ¨æé«˜æ”¶æ•›é€Ÿåº¦ã€ç¨³å®šæ€§å’Œä¼˜åŒ–æ•ˆæœã€‚
![image](https://github.com/user-attachments/assets/d397c91a-8037-48b2-a4cd-5746ad8cecaa)
![image](https://github.com/user-attachments/assets/927b7fc2-6a24-43bd-a62a-5ea02729dfb7)
![image](https://github.com/user-attachments/assets/5fdff66e-8fee-47ec-b985-4c767ac883f8)
![image](https://github.com/user-attachments/assets/dadc7dd9-1d0e-4911-bd19-c9c707aa212e)
![image](https://github.com/user-attachments/assets/2bb7a265-3aa7-4d9b-99c4-99f364d8028d)
![image](https://github.com/user-attachments/assets/bb1e90d6-e3d5-4d18-99bc-1cbfdd48d89f)
![image](https://github.com/user-attachments/assets/3fa80ab1-596a-4987-b5a7-afcb8e291430)
![image](https://github.com/user-attachments/assets/5841d2d8-4306-43a4-b129-fc72f92edf57)
![image](https://github.com/user-attachments/assets/46ca7274-a82c-4671-ba39-58d3690df9f5)


```python

def adam_optimizer(f, grad, x0, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8, num_iterations=10):
    x = x0
    m = np.zeros_like(x)
    v = np.zeros_like(x)

    for t in range(1, num_iterations + 1):
        g = grad(x)
        m = beta1 * m + (1 - beta1) * g
        v = beta2 * v + (1 - beta2) * g**2
        m_hat = m / (1 - beta1**t)
        v_hat = v / (1 - beta2**t)
        x = x - learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)
    return x

```

### 1.7 ä»€ä¹ˆæ˜¯ Dropoutï¼Ÿå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼ŸDropout çš„ä¿ç•™æ¦‚ç‡å¦‚ä½•é€‰æ‹©ï¼Ÿæ˜¯å¦å¯ä»¥åŠ¨æ€è°ƒæ•´ï¼Ÿ
- Dropout æ˜¯ä¸€ç§æ­£åˆ™åŒ–æŠ€æœ¯ï¼Œç”¨äºé˜²æ­¢ç¥ç»ç½‘ç»œçš„è¿‡æ‹Ÿåˆã€‚å®ƒé€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºâ€œä¸¢å¼ƒâ€ä¸€éƒ¨åˆ†ç¥ç»å…ƒï¼ˆå³å°†å…¶è¾“å‡ºç½®ä¸ºé›¶ï¼‰ï¼Œä»è€Œå‡å°‘ç¥ç»å…ƒä¹‹é—´çš„ç›¸äº’ä¾èµ–ï¼Œæé«˜æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ã€‚
- é€šå¸¸é€‰æ‹© 0.5ï¼Œä½†å¯ä»¥æ ¹æ®ä»»åŠ¡è°ƒæ•´ã€‚åŠ¨æ€ Dropoutï¼ˆå¦‚ DropConnectï¼‰å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´ä¿ç•™æ¦‚ç‡ã€‚

### 1.8 ä»€ä¹ˆæ˜¯ L1 å’Œ L2 æ­£åˆ™åŒ–ï¼Ÿå®ƒä»¬çš„åŒºåˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
L1 å’Œ L2 æ­£åˆ™åŒ–æ˜¯ä¸¤ç§å¸¸ç”¨çš„æ­£åˆ™åŒ–æ–¹æ³•ï¼Œç”¨äºé˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆã€‚å®ƒä»¬é€šè¿‡åœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡çš„æƒ©ç½šé¡¹ï¼Œé™åˆ¶æ¨¡å‹çš„å¤æ‚åº¦ã€‚
- L1 æ­£åˆ™åŒ–ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡çš„ç»å¯¹å€¼ä¹‹å’Œï¼Œä¿ƒä½¿æƒé‡ç¨€ç–åŒ–ï¼ˆå³éƒ¨åˆ†æƒé‡å˜ä¸ºé›¶ï¼‰ï¼Œé€‚åˆç‰¹å¾é€‰æ‹©ï¼Œé€‚ç”¨äºé«˜ç»´ç¨€ç–æ•°æ®
- L2 æ­£åˆ™åŒ–ï¼šåœ¨æŸå¤±å‡½æ•°ä¸­æ·»åŠ æƒé‡çš„å¹³æ–¹å’Œï¼Œä¿ƒä½¿æƒé‡è¶‹å‘äºè¾ƒå°çš„å€¼ï¼Œä½†ä¸ä¼šå®Œå…¨ä¸ºé›¶

## CNNæ‰‹æ’•åŠé¢è¯•
### 1. ä»€ä¹ˆæ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Ÿå®ƒä¸æ™®é€šç¥ç»ç½‘ç»œæœ‰ä½•ä¸åŒï¼Ÿ

ä¸æ™®é€šç¥ç»ç½‘ç»œï¼ˆå¦‚å…¨è¿æ¥ç½‘ç»œï¼‰ä¸åŒï¼ŒCNNåˆ©ç”¨äº†å›¾åƒçš„å±€éƒ¨ç›¸å…³æ€§å’Œå‚æ•°å…±äº«ï¼Œå‡å°‘äº†å‚æ•°æ•°é‡ï¼Œæé«˜äº†è®­ç»ƒæ•ˆç‡ã€‚

### 2. ä»€ä¹ˆæ˜¯å·ç§¯æ“ä½œï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å·ç§¯ï¼Ÿ
å·ç§¯æ“ä½œæ˜¯é€šè¿‡ä¸€ä¸ªå°çš„æ»¤æ³¢å™¨ï¼ˆkernelï¼‰åœ¨è¾“å…¥æ•°æ®ä¸Šæ»‘åŠ¨ï¼Œè®¡ç®—ç‚¹ç§¯ä»¥æå–å±€éƒ¨ç‰¹å¾ã€‚å·ç§¯çš„ä½œç”¨æ˜¯æå–å±€éƒ¨æ¨¡å¼ï¼ˆå¦‚è¾¹ç¼˜ã€çº¹ç†ç­‰ï¼‰ï¼Œå¹¶é€šè¿‡å±‚å±‚å åŠ æå–æ›´é«˜å±‚æ¬¡çš„ç‰¹å¾ã€‚

### 3. ä»€ä¹ˆæ˜¯æ± åŒ–ï¼ˆPoolingï¼‰ï¼Ÿæœ‰å“ªäº›å¸¸è§çš„æ± åŒ–æ–¹æ³•ï¼Ÿ

æ± åŒ–æ˜¯ä¸€ç§ä¸‹é‡‡æ ·æ“ä½œï¼Œç”¨äºå‡å°‘ç‰¹å¾å›¾çš„å°ºå¯¸ï¼Œé™ä½è®¡ç®—å¤æ‚åº¦ï¼ŒåŒæ—¶ä¿ç•™é‡è¦ç‰¹å¾ã€‚
å¸¸è§çš„æ± åŒ–æ–¹æ³•ï¼šæœ€å¤§æ± åŒ–ï¼ˆMax Poolingï¼‰ï¼šå–æ± åŒ–çª—å£ä¸­çš„æœ€å¤§å€¼ã€‚å¹³å‡æ± åŒ–ï¼ˆAverage Poolingï¼‰ï¼šå–æ± åŒ–çª—å£ä¸­çš„å¹³å‡å€¼

### 4. ä»€ä¹ˆæ˜¯å¡«å……ï¼ˆPaddingï¼‰ï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å¡«å……ï¼Ÿ
å¡«å……æ˜¯åœ¨è¾“å…¥æ•°æ®çš„è¾¹ç¼˜æ·»åŠ é¢å¤–çš„åƒç´ ï¼ˆé€šå¸¸ä¸º0ï¼‰ï¼Œä»¥æ§åˆ¶è¾“å‡ºç‰¹å¾å›¾çš„å¤§å°ã€‚
ä½œç”¨ï¼šä¿æŒè¾“å‡ºç‰¹å¾å›¾çš„å°ºå¯¸ï¼ˆ"same" paddingï¼‰ã€‚æé«˜è¾¹ç¼˜åŒºåŸŸçš„ç‰¹å¾æå–èƒ½åŠ›


###  5. ä»€ä¹ˆæ˜¯å‚æ•°å…±äº«ï¼Ÿä¸ºä»€ä¹ˆå®ƒå¯¹CNNå¾ˆé‡è¦ï¼Ÿ

å‚æ•°å…±äº«çš„æ ¸å¿ƒæ€æƒ³æ˜¯åˆ©ç”¨å›¾åƒçš„å¹³ç§»ä¸å˜æ€§ã€‚åŒä¸€ä¸ªå·ç§¯æ ¸åœ¨æ•´ä¸ªè¾“å…¥ç‰¹å¾å›¾ä¸Šæ»‘åŠ¨æ—¶ä½¿ç”¨ç›¸åŒçš„å‚æ•°ã€‚

### 6. CNNçš„æ„Ÿå—é‡ï¼ˆReceptive Fieldï¼‰æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

æ„Ÿå—é‡æ˜¯æŒ‡å·ç§¯ç¥ç»ç½‘ç»œä¸­æŸä¸€å±‚çš„ä¸€ä¸ªç¥ç»å…ƒåœ¨è¾“å…¥å›¾åƒä¸Šå¯¹åº”çš„åŒºåŸŸå¤§å°ã€‚æ„Ÿå—é‡è¶Šå¤§ï¼Œç¥ç»å…ƒèƒ½å¤Ÿæ•è·çš„ä¸Šä¸‹æ–‡ä¿¡æ¯è¶Šå¤šã€‚
æ·±å±‚ç½‘ç»œé€šè¿‡å åŠ å·ç§¯å±‚å¯ä»¥æ‰©å¤§æ„Ÿå—é‡ï¼Œä»è€Œæå–å…¨å±€ç‰¹å¾

### 7.ä»€ä¹ˆæ˜¯è½¬ç½®å·ç§¯ï¼ˆTransposed Convolutionï¼‰ï¼Ÿå®ƒçš„ä½œç”¨æ˜¯ä»€ä¹ˆï¼Ÿ
è½¬ç½®å·ç§¯æ˜¯ä¸€ç§ä¸Šé‡‡æ ·æ“ä½œï¼Œç”¨äºå°†ä½åˆ†è¾¨ç‡ç‰¹å¾å›¾æ¢å¤åˆ°é«˜åˆ†è¾¨ç‡ã€‚
ä½œç”¨ï¼šå¸¸ç”¨äºç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰å’Œå›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ï¼Œç”¨äºç”Ÿæˆé«˜åˆ†è¾¨ç‡å›¾åƒæˆ–æ¢å¤åŸå§‹å°ºå¯¸ã€‚
è½¬ç½®å·ç§¯å¹¶ä¸æ˜¯ç®€å•çš„åå‘å·ç§¯ï¼Œè€Œæ˜¯é€šè¿‡æ’å€¼å’Œå·ç§¯æ“ä½œå®ç°ä¸Šé‡‡æ ·ã€‚


### 8. CNNä¸­å¸¸è§çš„ç½‘ç»œæ¶æ„æœ‰å“ªäº›ï¼Ÿ
- AlexNetï¼šå¼•å…¥ReLUã€Dropoutå’Œæ•°æ®å¢å¼ºï¼Œèµ¢å¾—ImageNetæ¯”èµ›ã€‚
- VGGï¼šä½¿ç”¨å°å·ç§¯æ ¸ï¼ˆ3x3ï¼‰å †å ï¼Œç»“æ„ç®€å•ä½†å‚æ•°é‡å¤§ã€‚
- GoogLeNetï¼ˆInceptionï¼‰ï¼šå¼•å…¥Inceptionæ¨¡å—ï¼Œå‡å°‘è®¡ç®—é‡ã€‚åœ¨åŒä¸€å±‚ä¸­å¹¶è¡Œä½¿ç”¨ä¸åŒå¤§å°çš„å·ç§¯æ ¸ï¼ˆå¦‚1Ã—1ã€3Ã—3ã€5Ã—5ï¼‰å’Œæ± åŒ–æ“ä½œã€‚é€šè¿‡å¤šå°ºåº¦ç‰¹å¾æå–ï¼Œæ•è·ä¸åŒå¤§å°çš„ç‰¹å¾ã€‚
- ResNetï¼šå¼•å…¥æ®‹å·®è¿æ¥ï¼ˆskip connectionï¼‰ï¼Œè§£å†³æ·±å±‚ç½‘ç»œçš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚æ®‹å·®çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼šè®©ç½‘ç»œå­¦ä¹ è¾“å…¥ä¸è¾“å‡ºä¹‹é—´çš„å·®å€¼ï¼ˆæ®‹å·®ï¼‰ï¼Œè€Œä¸æ˜¯ç›´æ¥å­¦ä¹ è¾“å‡ºæœ¬èº«ã€‚æ®‹å·®è¿æ¥ä½¿å¾—ç½‘ç»œæ›´å®¹æ˜“å­¦ä¹ åˆ°æ’ç­‰æ˜ å°„æˆ–æ¥è¿‘æ’ç­‰æ˜ å°„çš„å‡½æ•°ã€‚
å³ä½¿å¢åŠ äº†ç½‘ç»œæ·±åº¦ï¼Œæ–°çš„å±‚å¯ä»¥é€‰æ‹©å­¦ä¹ â€œé›¶æ˜ å°„â€ï¼ˆå³ä¸æ”¹å˜è¾“å…¥ï¼‰ï¼Œä»è€Œé¿å…é€€åŒ–é—®é¢˜ã€‚
- ç°ä»£æ¶æ„å¦‚EfficientNetï¼ˆåŒæ—¶è°ƒæ•´ç½‘ç»œçš„æ·±åº¦ã€å®½åº¦å’Œåˆ†è¾¨ç‡ï¼Œæ‰¾åˆ°æœ€ä¼˜çš„ç¼©æ”¾æ¯”ä¾‹ï¼‰ã€MobileNetï¼ˆå°†æ ‡å‡†å·ç§¯åˆ†è§£ä¸ºæ·±åº¦å·ç§¯ï¼ˆDepthwise Convolutionï¼‰å’Œé€ç‚¹å·ç§¯ï¼ˆPointwise Convolutionï¼‰ï¼‰ç­‰ï¼Œæ³¨é‡å‚æ•°æ•ˆç‡å’Œè®¡ç®—æ•ˆç‡

### 9. CNNçš„å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ³•
é—®é¢˜1ï¼šè¿‡æ‹Ÿåˆï¼šæ•°æ®å¢å¼ºï¼ˆå¦‚ç¿»è½¬ã€æ—‹è½¬ã€è£å‰ªï¼‰ã€‚æ­£åˆ™åŒ–ï¼ˆå¦‚L2æ­£åˆ™åŒ–ã€Dropoutï¼‰ã€‚
é—®é¢˜2ï¼šæ¢¯åº¦æ¶ˆå¤±æˆ–æ¢¯åº¦çˆ†ç‚¸ï¼šä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°ã€‚ä½¿ç”¨æ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰æˆ–æ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰ã€‚

### 10. æ‰‹æ’•äºŒç»´å·ç§¯
```python
import numpy as np  

def simple_conv2d(input_matrix: np.ndarray, kernel: np.ndarray, padding: int, stride: int):  
    """  
    å®ç°ä¸€ä¸ªç®€å•çš„2Då·ç§¯æ“ä½œã€‚  

    å‚æ•°ï¼š  
    - input_matrix: è¾“å…¥çŸ©é˜µï¼ˆäºŒç»´æ•°ç»„ï¼‰ï¼Œè¡¨ç¤ºè¾“å…¥çš„å›¾åƒæˆ–ç‰¹å¾å›¾ã€‚  
    - kernel: å·ç§¯æ ¸ï¼ˆäºŒç»´æ•°ç»„ï¼‰ï¼Œç”¨äºæå–ç‰¹å¾ã€‚  
    - padding: å¡«å……å¤§å°ï¼Œåœ¨è¾“å…¥çŸ©é˜µçš„è¾¹ç¼˜æ·»åŠ çš„é›¶çš„æ•°é‡ã€‚  
    - stride: æ­¥å¹…ï¼Œå·ç§¯æ ¸åœ¨è¾“å…¥çŸ©é˜µä¸Šæ»‘åŠ¨çš„æ­¥é•¿ã€‚  

    è¿”å›ï¼š  
    - output_matrix: è¾“å‡ºçŸ©é˜µï¼ˆäºŒç»´æ•°ç»„ï¼‰ï¼Œè¡¨ç¤ºå·ç§¯æ“ä½œåçš„ç‰¹å¾å›¾ã€‚  
    """  
    # è·å–è¾“å…¥çŸ©é˜µçš„é«˜åº¦å’Œå®½åº¦  
    input_height, input_width = input_matrix.shape  

    # è·å–å·ç§¯æ ¸çš„é«˜åº¦å’Œå®½åº¦  
    kernel_height, kernel_width = kernel.shape  

    # å¯¹è¾“å…¥çŸ©é˜µè¿›è¡Œå¡«å……ï¼Œå¡«å……æ¨¡å¼ä¸ºå¸¸æ•°ï¼ˆå€¼ä¸º0ï¼‰  
    # å¡«å……çš„å¤§å°ä¸º (padding, padding) åœ¨é«˜åº¦å’Œå®½åº¦æ–¹å‘ä¸Šåˆ†åˆ«æ·»åŠ   
    padded_input = np.pad(input_matrix, ((padding, padding), (padding, padding)), mode='constant')  

    # è·å–å¡«å……åçš„è¾“å…¥çŸ©é˜µçš„é«˜åº¦å’Œå®½åº¦  
    input_height_padded, input_width_padded = padded_input.shape  

    # è®¡ç®—è¾“å‡ºçŸ©é˜µçš„é«˜åº¦å’Œå®½åº¦  
    # è¾“å‡ºå°ºå¯¸å…¬å¼ï¼š((è¾“å…¥å°ºå¯¸ + 2*å¡«å…… - å·ç§¯æ ¸å°ºå¯¸) // æ­¥å¹…) + 1  
    output_height = (input_height_padded - kernel_height) // stride + 1  
    output_width = (input_width_padded - kernel_width) // stride + 1  

    # åˆå§‹åŒ–è¾“å‡ºçŸ©é˜µï¼Œå¤§å°ä¸º (output_height, output_width)ï¼Œåˆå§‹å€¼ä¸º0  
    output_matrix = np.zeros((output_height, output_width))  

    # éå†è¾“å‡ºçŸ©é˜µçš„æ¯ä¸ªä½ç½®  
    for i in range(output_height):  # éå†è¾“å‡ºçŸ©é˜µçš„è¡Œ  
        for j in range(output_width):  # éå†è¾“å‡ºçŸ©é˜µçš„åˆ—  
            # æå–è¾“å…¥çŸ©é˜µä¸­ä¸å½“å‰å·ç§¯æ ¸ä½ç½®å¯¹åº”çš„åŒºåŸŸ  
            # åŒºåŸŸçš„èµ·å§‹ä½ç½®ç”± (i*stride, j*stride) å†³å®š  
            # åŒºåŸŸçš„å¤§å°ä¸å·ç§¯æ ¸ç›¸åŒ  
            region = padded_input[i*stride:i*stride + kernel_height, j*stride:j*stride + kernel_width]  
            
            # è®¡ç®—åŒºåŸŸä¸å·ç§¯æ ¸çš„é€å…ƒç´ ä¹˜ç§¯çš„å’Œï¼Œå¹¶èµ‹å€¼ç»™è¾“å‡ºçŸ©é˜µçš„å½“å‰ä½ç½®  
            output_matrix[i, j] = np.sum(region * kernel)  

    # è¿”å›å·ç§¯æ“ä½œåçš„è¾“å‡ºçŸ©é˜µ  
    return output_matrix
```

## RNNæ‰‹æ’•åŠé¢è¯•
![image](https://github.com/user-attachments/assets/83395ace-d874-447b-8cb1-2e7f0494f93f)
![image](https://github.com/user-attachments/assets/9f687b18-034f-4b78-9112-c618e62888d5)
![image](https://github.com/user-attachments/assets/b3ae7939-2bcb-423e-9c00-7e78cdb1d987)
![image](https://github.com/user-attachments/assets/0a891299-0747-48cb-892a-2c784141627c)
![image](https://github.com/user-attachments/assets/5e636d51-d093-4360-9f0b-22d656cf5ca6)
![image](https://github.com/user-attachments/assets/ea2cc8b0-c47b-4316-a155-dc92cc3a6e8d)
```python
def rnn_forward(input_sequence, initial_hidden_state, Wx, Wh, b):
    h = np.array(initial_hidden_state)
    Wx = np.array(Wx)
    Wh = np.array(Wh)
    b = np.array(b)
    for x in input_sequence:
        x = np.array(x)
        h = np.tanh(np.dot(Wx, x) + np.dot(Wh, h) + b)
    final_hidden_state = np.round(h, 4)
    return final_hidden_state.tolist()
```
```python
class LSTM:  
    def __init__(self, input_size, hidden_size):  
        """  
        åˆå§‹åŒ– LSTM çš„æƒé‡å’Œåç½®ã€‚  

        å‚æ•°ï¼š  
        - input_size: è¾“å…¥ç‰¹å¾çš„ç»´åº¦ã€‚  
        - hidden_size: éšè—çŠ¶æ€çš„ç»´åº¦ï¼ˆå³ LSTM å•å…ƒçš„æ•°é‡ï¼‰ã€‚  
        """  
        self.input_size = input_size  
        self.hidden_size = hidden_size  

        # åˆå§‹åŒ–é—å¿˜é—¨ (Forget Gate) çš„æƒé‡å’Œåç½®  
        self.Wf = np.random.randn(hidden_size, input_size + hidden_size)  # é—å¿˜é—¨çš„æƒé‡çŸ©é˜µ  
        self.bf = np.zeros((hidden_size, 1))  # é—å¿˜é—¨çš„åç½®  

        # åˆå§‹åŒ–è¾“å…¥é—¨ (Input Gate) çš„æƒé‡å’Œåç½®  
        self.Wi = np.random.randn(hidden_size, input_size + hidden_size)  # è¾“å…¥é—¨çš„æƒé‡çŸ©é˜µ  
        self.bi = np.zeros((hidden_size, 1))  # è¾“å…¥é—¨çš„åç½®  

        # åˆå§‹åŒ–å€™é€‰è®°å¿†å•å…ƒ (Candidate Cell State) çš„æƒé‡å’Œåç½®  
        self.Wc = np.random.randn(hidden_size, input_size + hidden_size)  # å€™é€‰è®°å¿†å•å…ƒçš„æƒé‡çŸ©é˜µ  
        self.bc = np.zeros((hidden_size, 1))  # å€™é€‰è®°å¿†å•å…ƒçš„åç½®  

        # åˆå§‹åŒ–è¾“å‡ºé—¨ (Output Gate) çš„æƒé‡å’Œåç½®  
        self.Wo = np.random.randn(hidden_size, input_size + hidden_size)  # è¾“å‡ºé—¨çš„æƒé‡çŸ©é˜µ  
        self.bo = np.zeros((hidden_size, 1))  # è¾“å‡ºé—¨çš„åç½®  

    def forward(self, x, initial_hidden_state, initial_cell_state):  
        """  
        å‰å‘ä¼ æ’­ï¼Œè®¡ç®— LSTM çš„è¾“å‡ºã€‚  

        å‚æ•°ï¼š  
        - x: è¾“å…¥åºåˆ—ï¼Œå½¢çŠ¶ä¸º (åºåˆ—é•¿åº¦, è¾“å…¥ç‰¹å¾ç»´åº¦)ã€‚  
        - initial_hidden_state: åˆå§‹éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º (hidden_size, 1)ã€‚  
        - initial_cell_state: åˆå§‹è®°å¿†å•å…ƒçŠ¶æ€ï¼Œå½¢çŠ¶ä¸º (hidden_size, 1)ã€‚  

        è¿”å›ï¼š  
        - outputs: æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º (åºåˆ—é•¿åº¦, hidden_size, 1)ã€‚  
        - h: æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€ã€‚  
        - c: æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è®°å¿†å•å…ƒçŠ¶æ€ã€‚  
        """  
        h = initial_hidden_state  # å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€  
        c = initial_cell_state  # å½“å‰æ—¶é—´æ­¥çš„è®°å¿†å•å…ƒçŠ¶æ€  
        outputs = []  # ç”¨äºå­˜å‚¨æ¯ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€  

        # éå†è¾“å…¥åºåˆ—çš„æ¯ä¸ªæ—¶é—´æ­¥  
        for t in range(len(x)):  
            xt = x[t].reshape(-1, 1)  # å½“å‰æ—¶é—´æ­¥çš„è¾“å…¥ï¼Œå½¢çŠ¶ä¸º (input_size, 1)  
            concat = np.vstack((h, xt))  # å°†éšè—çŠ¶æ€å’Œè¾“å…¥æ‹¼æ¥ï¼Œå½¢çŠ¶ä¸º (hidden_size + input_size, 1)  

            # é—å¿˜é—¨ (Forget Gate)  
            # è®¡ç®—é—å¿˜é—¨çš„æ¿€æ´»å€¼ï¼Œå†³å®šè®°å¿†å•å…ƒä¸­å“ªäº›ä¿¡æ¯éœ€è¦é—å¿˜  
            ft = self.sigmoid(np.dot(self.Wf, concat) + self.bf)  

            # è¾“å…¥é—¨ (Input Gate)  
            # è®¡ç®—è¾“å…¥é—¨çš„æ¿€æ´»å€¼ï¼Œå†³å®šå½“å‰è¾“å…¥ä¸­å“ªäº›ä¿¡æ¯éœ€è¦å†™å…¥è®°å¿†å•å…ƒ  
            it = self.sigmoid(np.dot(self.Wi, concat) + self.bi)  
            # è®¡ç®—å€™é€‰è®°å¿†å•å…ƒçš„å€¼ï¼Œè¡¨ç¤ºå½“å‰æ—¶é—´æ­¥çš„æ–°ä¿¡æ¯  
            c_tilde = np.tanh(np.dot(self.Wc, concat) + self.bc)  

            # æ›´æ–°è®°å¿†å•å…ƒçŠ¶æ€ (Cell State)  
            # é—å¿˜é—¨æ§åˆ¶é—å¿˜æ—§ä¿¡æ¯ï¼Œè¾“å…¥é—¨æ§åˆ¶å†™å…¥æ–°ä¿¡æ¯  
            c = ft * c + it * c_tilde  

            # è¾“å‡ºé—¨ (Output Gate)  
            # è®¡ç®—è¾“å‡ºé—¨çš„æ¿€æ´»å€¼ï¼Œå†³å®šè®°å¿†å•å…ƒä¸­å“ªäº›ä¿¡æ¯éœ€è¦è¾“å‡º  
            ot = self.sigmoid(np.dot(self.Wo, concat) + self.bo)  

            # æ›´æ–°éšè—çŠ¶æ€ (Hidden State)  
            # éšè—çŠ¶æ€ç”±è¾“å‡ºé—¨æ§åˆ¶ï¼Œå¹¶ç»“åˆå½“å‰è®°å¿†å•å…ƒçš„å€¼  
            h = ot * np.tanh(c)  

            # å°†å½“å‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€å­˜å‚¨åˆ°è¾“å‡ºåˆ—è¡¨ä¸­  
            outputs.append(h)  

        # è¿”å›æ‰€æœ‰æ—¶é—´æ­¥çš„éšè—çŠ¶æ€ï¼Œä»¥åŠæœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„éšè—çŠ¶æ€å’Œè®°å¿†å•å…ƒçŠ¶æ€  
        return np.array(outputs), h, c  

    def sigmoid(self, x):  
        """  
        Sigmoid æ¿€æ´»å‡½æ•°ã€‚  

        å‚æ•°ï¼š  
        - x: è¾“å…¥å€¼ã€‚  

        è¿”å›ï¼š  
        - Sigmoid å‡½æ•°çš„è¾“å‡ºå€¼ã€‚  
        """  
        return 1 / (1 + np.exp(-x))  
```


## Transformeræ‰‹æ’•åŠé¢è¯•
- [Transformer1](https://zhuanlan.zhihu.com/p/438625445)  
- [Transformer2](https://zhuanlan.zhihu.com/p/363466672)  
- [Transformer3](https://zhuanlan.zhihu.com/p/148656446)

```python
import torch
import math
import torch.nn as nn


class Embeddings(nn.Module):
    def __init__(self, d_model, vocab):
        """
        :param d_model: è¯åµŒå…¥çš„ç»´åº¦
        :param vocab: è¯è¡¨çš„å¤§å°
        """
        super(Embeddings, self).__init__()

        self.lut = nn.Embedding(vocab, d_model)
        self.d_model = d_model

    def forward(self, x):
        x = self.lut(x)

        return x * math.sqrt(self.d_model)
```

```python
import torch
import torch.nn as nn
import math


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len):
        super().__init__()
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
        pe = torch.zeros(max_len, d_model)

        # åˆ›å»ºä½ç½®çŸ©é˜µ
        position = torch.arange(0, max_len).unsqueeze(1)  # [max_len, 1]

        # åˆ›å»ºé™¤æ•°é¡¹
        div_term = torch.exp(torch.arange(0, d_model, 2) * -math.log(10000) / d_model)

        # è®¡ç®—ä½ç½®ç¼–ç 
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦å¹¶æ³¨å†Œä¸ºbuffer
        pe = pe.unsqueeze(0)  # [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        å‚æ•°:
            x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
        """
        x = x + self.pe[:, :x.size(1)]
        return x
```

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_head):
        super(MultiHeadAttention, self).__init__()

        self.d_model = d_model
        self.n_head = n_head
        self.head_dim = d_model // n_head


        self.wq = nn.Linear(d_model, d_model)
        self.wk = nn.Linear(d_model, d_model)
        self.wv = nn.Linear(d_model, d_model)

        self.out_linear = nn.Linear(d_model, d_model)

    def forward(self, query, key, value, mask=None):

        batch_size, seq_length = query.size(0), query.size(1)

        Q = self.wq(query)
        K = self.wk(key)
        V = self.wv(value)

        Q = Q.view(batch_size, seq_length, self.n_head, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, seq_length, self.n_head, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, seq_length, self.n_head, self.head_dim).transpose(1, 2)

        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_dim ** 0.5)

        if mask is not None:
            mask = mask.unsqueeze(1)
            mask = mask.bool()
            scores = scores.masked_fill(~mask, float('-inf'))

        attention_weight = F.softmax(scores, dim = -1)

        att_out = torch.matmul(attention_weight, V)
        att_out = att_out.transpose(1, 2).contiguous()
        att_out = att_out.view(batch_size, seq_length, self.d_model)

        output = self.out_linear(att_out)

        return output
if __name__ == "__main__":
    d_model = 512
    n_head = 8
    batch_size = 16
    seq_length = 32
    attention = MultiHeadAttention(d_model, n_head)
    query = torch.rand(batch_size, seq_length, d_model)
    key = torch.rand(batch_size, seq_length, d_model)
    value = torch.rand(batch_size, seq_length, d_model)
    output = attention(query, key, value)
    print("è¾“å…¥å½¢çŠ¶:", query.shape)
    print("è¾“å‡ºå½¢çŠ¶:", output.shape)
```

```python
import torch
import torch.nn as nn

class FeedForward(nn.Module):
    def __init__(self, d_model, hidden_dim, dropout = 0.1):
        super(FeedForward, self).__init__()
        self.d_model = d_model
        self.hidden_dim = hidden_dim

        self.fc1 = nn.Linear(self.d_model, self.hidden_dim)
        self.fc2 = nn.Linear(self.hidden_dim, self.d_model)
        self.dropout = nn.Dropout(dropout)

        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)


        return x

```

```python
import torch
import torch.nn as nn

class LayerNorm(nn.Module):
    def __init__(self, d_model, eps = 1e-12):
        super(LayerNorm, self).__init__()
        self.gamma = nn.Parameter(torch.ones(d_model))
        self.beta = nn.Parameter(torch.ones(d_model))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim = True)
        var = x.var(-1, unbiased = False, keepdim = True)
        out = (x - mean) / torch.sqrt(var + self.eps)
        out = self.gamma * out + self.beta
        return out
```

## VAEå˜åˆ†è‡ªç¼–ç å™¨æ¨å¯¼

- AEï¼šAutoencoderæ˜¯ä¸€ç§è‡ªç›‘ç£çš„ç¥ç»ç½‘ç»œï¼Œç”¨äºå­¦ä¹ æ•°æ®çš„é«˜æ•ˆè¡¨ç¤ºã€‚å…¶ä¸»è¦ç›®æ ‡æ˜¯é€šè¿‡å‹ç¼©æ•°æ®å¹¶å°è¯•é‡æ„å®ƒæ¥æ•æ‰æ•°æ®çš„å…³é”®ç‰¹å¾ç”±äºAutoencoderï¼ˆè‡ªç¼–ç å™¨ï¼‰å­¦ä¹ çš„æ˜¯æ’ç­‰å‡½æ•°ï¼Œå½“ç½‘ç»œçš„å‚æ•°æ•°é‡è¶…è¿‡æ•°æ®æœ¬èº«çš„å¤æ‚åº¦æ—¶ï¼Œå­˜åœ¨è¿‡æ‹Ÿåˆçš„é£é™©ã€‚
- VAEï¼šåœ¨VAEä¸­ï¼Œè¾“å…¥æ•°æ®è¢«æ˜ å°„åˆ°ä¸€ä¸ªæ½œåœ¨çš„éšå‘é‡çš„åˆ†å¸ƒä¸Šï¼Œè¿™ä¸ªåˆ†å¸ƒé€šå¸¸å‡è®¾ä¸ºæ­£æ€åˆ†å¸ƒï¼Œå…¶å‚æ•°ç”±è¾“å…¥æ•°æ®å†³å®šã€‚å› æ­¤ï¼ŒVAEçš„å…³é”®åœ¨äºå­¦ä¹ è¾“å…¥æ•°æ®çš„æ¦‚ç‡åˆ†å¸ƒç‰¹æ€§ï¼Œè€Œä¸ä»…ä»…æ˜¯ç¡®å®šæ€§çš„æ˜ å°„å…³ç³»ã€‚ï¼ˆå› ä¸ºé‡‡æ ·çš„å‚æ•°å’Œç½‘ç»œå‚æ•°ç»‘å®šåœ¨ä¸€èµ·ï¼Œå› æ­¤éœ€è¦é‡å‚æ•°åŒ–æŠ€å·§ï¼Œé€šä¿—ç‚¹å°±æ˜¯ç¥ç»ç½‘ç»œè¾“å‡ºè¿‘ä¼¼åéªŒåˆ†å¸ƒçš„å‡å€¼å’Œæ–¹å·®ï¼‰
- VQ-VAEï¼ˆVector Quantized Variational Autoencoderï¼‰æ˜¯ä¸€ç§åŸºäºç¦»æ•£æ½œåœ¨ç©ºé—´çš„ç”Ÿæˆæ¨¡å‹ï¼Œå®ƒç»“åˆäº†å˜åˆ†è‡ªåŠ¨ç¼–ç å™¨ï¼ˆVAEï¼‰å’ŒçŸ¢é‡é‡åŒ–ï¼ˆVector Quantizationï¼‰çš„æ€æƒ³ã€‚VQ-VAE çš„æ ¸å¿ƒç›®æ ‡æ˜¯ç”¨ç¦»æ•£çš„æ½œåœ¨è¡¨ç¤ºæ¥å¯¹æ•°æ®è¿›è¡Œå»ºæ¨¡ï¼ŒåŒæ—¶é¿å…ä¼ ç»Ÿ VAE åœ¨æ½œåœ¨ç©ºé—´è¿ç»­åˆ†å¸ƒä¸Šçš„æ¨¡ç³Šæ€§é—®é¢˜ã€‚åœæ­¢æ¢¯åº¦åœ¨ VQ-VAE ä¸­çš„æ„ä¹‰ï¼š1. è§£å†³ç¦»æ•£æ“ä½œä¸å¯å¯¼çš„é—®é¢˜ï¼šç¦»æ•£æ“ä½œï¼ˆå¦‚çŸ¢é‡é‡åŒ–ï¼‰é€šè¿‡åœæ­¢æ¢¯åº¦è®©æ¢¯åº¦æµä»…ä½œç”¨äºè¿ç»­å˜é‡ï¼Œä»è€Œå®ç°æ¨¡å‹çš„ç«¯åˆ°ç«¯ä¼˜åŒ–ã€‚2. åˆ†ç¦»ç¼–ç å™¨å’Œä»£ç è¡¨çš„ä¼˜åŒ–ï¼šåœæ­¢æ¢¯åº¦è®©ç¼–ç å™¨çš„è¾“å‡º $z_e(x)$ å’Œä»£ç è¡¨çš„åµŒå…¥ $e_k$ ä¼˜åŒ–è¿‡ç¨‹è§£è€¦ï¼Œä½¿ä¸¤è€…èƒ½å¤Ÿç‹¬ç«‹è®­ç»ƒï¼Œæœ€ç»ˆæ”¶æ•›åˆ°åˆç†çš„è¡¨ç¤ºã€‚
![image](https://github.com/user-attachments/assets/4ba95eea-c9ab-4a89-af81-7793c2d8bca2)
![image](https://github.com/user-attachments/assets/e508c84a-27a4-4554-a365-b98fe78d864d)
![image](https://github.com/user-attachments/assets/7d2cafd5-2876-4597-a300-f4dccd5deec5)
![image](https://github.com/user-attachments/assets/916f93e3-44a0-4ca3-a019-43395099035f)
![image](https://github.com/user-attachments/assets/9c022246-b226-44d0-ac83-b662b0990dd6)

## GANç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¨å¯¼
![image](https://github.com/user-attachments/assets/8d384490-5f9c-4ad5-b4c8-f2b5171da1a8)
![image](https://github.com/user-attachments/assets/5ec9bee4-2e65-404e-81d5-1b2f477c14dc)
![image](https://github.com/user-attachments/assets/dc3e5455-22ab-4273-96a2-a2a2b2988f10)
![image](https://github.com/user-attachments/assets/3fe144f3-5199-43aa-9bd9-6216687072bf)

## Diffusion Model æ¨å¯¼

è§è®ºæ–‡ï¼š[Understanding Diffusion Models: A Unified Perspective](https://arxiv.org/abs/2208.11970)

## Diffusion Policy æ¨å¯¼


